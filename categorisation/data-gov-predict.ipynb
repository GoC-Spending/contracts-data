{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow_text\n!pip install pandas\n!pip install --upgrade tensorflow-hub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# need to reload the custom layers so they can be passed to our model on load\n# finetuning functions\nfrom keras import backend as K\n\nfrom keras.callbacks import ModelCheckpoint\n\ndef balanced_recall(y_true, y_pred):\n    \"\"\"This function calculates the balanced recall metric\n    recall = TP / (TP + FN)\n    \"\"\"\n    recall_by_class = 0\n    # iterate over each predicted class to get class-specific metric\n    for i in range(y_pred.shape[1]):\n        y_pred_class = y_pred[:, i]\n        y_true_class = y_true[:, i]\n        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        recall_by_class = recall_by_class + recall\n    return recall_by_class / y_pred.shape[1]\n\ndef balanced_precision(y_true, y_pred):\n    \"\"\"This function calculates the balanced precision metric\n    precision = TP / (TP + FP)\n    \"\"\"\n    precision_by_class = 0\n    # iterate over each predicted class to get class-specific metric\n    for i in range(y_pred.shape[1]):\n        y_pred_class = y_pred[:, i]\n        y_true_class = y_true[:, i]\n        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        precision_by_class = precision_by_class + precision\n    # return average balanced metric for each class\n    return precision_by_class / y_pred.shape[1]\n\ndef balanced_f1_score(y_true, y_pred):\n    \"\"\"This function calculates the F1 score metric\"\"\"\n    precision = balanced_precision(y_true, y_pred)\n    recall = balanced_recall(y_true, y_pred)\n    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imports and function/variable setting\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nimport re\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom string import punctuation\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\npunctuation = list(punctuation)\n\n# these characters are reminants of typos\npunctuation.extend([\"''\", '``'])\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n\ndef clean_desc(desc):\n    if type(desc) is str:\n        tokens = word_tokenize(desc.lower())\n\n        stwords = stopwords.words('english')\n        newDesc = [token for token in tokens if token not in stwords and token not in punctuation]\n        \n        return newDesc\n\nclass_keys = {'1_facilities_and_construction': 0,\n              '2_professional_services': 1,\n              '3_information_technology': 2,\n              '4_medical': 3,\n              '5_transportation_and_logistics': 4,\n              '6_industrial_products_and_services': 5,\n              '7_travel': 6,\n              '8_security_and_protection': 7,\n              '9_human_capital': 8,\n              '10_office_management': 9,\n              '11_defence': 10}\n\n# load dataset for creating test_set\nog_data = pd.read_csv('../input/datagovclassifier/rule_based_contracts_v1.csv', dtype='str', index_col=0)\n\n\n# load model\nmodel = keras.models.load_model((\"../input/datagovclassifier/BEST-n1-weights-improvement-22-0.92.hdf5\"), custom_objects={'KerasLayer':hub.KerasLayer,\n                                                                                                 'balanced_recall': balanced_recall, \n                                                                                                 'balanced_precision': balanced_precision, \n                                                                                                 'balanced_f1_score': balanced_f1_score})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keywords for Low Confidence Score","metadata":{}},{"cell_type":"code","source":"unique_classifiers = [c for c in og_data['category'].unique() if str(c) != 'nan' and len(str(c)) > 1]\n\nunique_classifiers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cate_keywords = {}\nfor c in unique_classifiers:\n    class_df = og_data.loc[og_data['category'] == c]\n\n    class_ls = class_df['description_en'].tolist()\n\n    class_str = ' '.join(str(w) for w in class_ls).lower()\n    class_str = clean_desc(class_str)\n    class_str = [t for t in class_str if t.isalpha()]\n\n    dist = FreqDist(class_str)\n\n    word_freq = [ws[0] for ws in dist.most_common(30)]\n    \n    cate_keywords[c] = word_freq\n    print(c)\n    print(cate_keywords[c])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# identify duplicates in keyword lists so they can be removed\n\n# using sets to ensure no duplicates\ndistinct = set()\nduplicate = set()\n\nfor k, v in cate_keywords.items():\n  for i in set(v):\n    if i in distinct:\n      duplicate.add(i)\n    else:\n      distinct.add(i)\n\nprint(distinct)\nprint(duplicate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove duplicates from keyword lists\nfor k, v in cate_keywords.items():\n    print(k)\n    print(v)\n    intersection = set(v) - duplicate\n    print(intersection)\n    print('------------------')\n    cate_keywords[k] = list(intersection)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply to Dataset","metadata":{}},{"cell_type":"code","source":"unclassified = og_data.loc[og_data['category'].isna() & og_data['description_en'].notna()].copy()\nunclassified","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor i in unclassified['description_en']:\n    desc = []\n    desc.append(i)\n    predictions.append(desc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, pred in enumerate(model.predict(unclassified['description_en'])):\n    predictions[i].extend([pred, \n                           list(class_keys)[np.argmax(pred)], \n                           list(class_keys)[np.argsort(pred, axis=0)[-2]],\n                           np.max(pred) * 100])    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# additional catch to compensate for lower confidence score \nverified_predictions = []\nfor pred in predictions:\n    if pred[4] <= 70:\n        desc = clean_desc(pred[0])\n\n        curr_cate = cate_keywords[pred[2]]\n        alt_cate = cate_keywords[pred[3]]\n\n        curr_matches = set(desc) & set(curr_cate)\n        alt_matches = set(desc) & set(alt_cate)\n\n        # const stands for construction --> consistently misclassified as IT\n        if 'const' in desc:\n            verified_predictions.append('1_facilities_and_construction')\n        # ignoring defence due to wide scope\n        elif len(curr_matches) < len(alt_matches) and pred[3] != '11_defence':\n            verified_predictions.append(pred[3])\n        else:\n            verified_predictions.append(pred[2])\n    else:\n        verified_predictions.append(pred[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unclassified['category'] = verified_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# putting combined dataframes into a temp dataframe bc combine_first rearranges column into alphabetical order???\ntemp_df = og_data.combine_first(unclassified)\ntemp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"og_data['category'] = temp_df['category']\n\nog_data.iloc[18504]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all indices for rows classified using the classifier\nautocat_index = []\nfor i, row in unclassified.iterrows():\n    autocat_index.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# label classified rows in dataframe\nog_data.insert(7,'auto_classified', 'False')\nog_data.loc[autocat_index, 'auto_classified'] = True\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"og_data.to_csv(\"classified_contracts.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}