{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n"
      ],
      "metadata": {
        "id": "fxrZGiyQZzmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vh1m4JML8T-D"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDlRVVZYmNI5"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "print(tf.__version__)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found {}'.format(device_name))\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVUfkm5s9J4J"
      },
      "outputs": [],
      "source": [
        "# connect to drive if in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "%cd gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8_9IYfp-1mn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read csv\n",
        "og_data = pd.read_csv('rule_based_contracts_v1.csv', dtype='str')\n",
        "og_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43_rjYQABgao"
      },
      "outputs": [],
      "source": [
        "# rm uncategorized rows from dataset so it can be used for training\n",
        "training_data = og_data[og_data['category'].notna()]\n",
        "\n",
        "# rm rows with no description for training\n",
        "training_data = training_data[training_data['description_en'].notna()]\n",
        "\n",
        "# minimize\n",
        "training_data = training_data[['category', 'description_en']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0A9PTb0APWj"
      },
      "outputs": [],
      "source": [
        "# checking current distribution \n",
        "# wow it's uneven lol this may cause problems\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "num_classes = len(training_data[\"category\"].value_counts())\n",
        "\n",
        "colors = plt.cm.Dark2(np.linspace(0, 1, num_classes))\n",
        "iter_color = iter(colors)\n",
        "\n",
        "training_data[\"category\"].value_counts().plot.barh(title=\"Reviews for each topic (n, %)\", \n",
        "                                                 ylabel=\"Topics\",\n",
        "                                                 color=colors,\n",
        "                                                 figsize=(9,9))\n",
        "\n",
        "for i, v in enumerate(training_data[\"category\"].value_counts()):\n",
        "  c = next(iter_color)\n",
        "  plt.text(v, i,\n",
        "           \" \"+str(v)+\", \"+str(round(v*100/training_data.shape[0],2))+\"%\", \n",
        "           color=c, \n",
        "           va='center', \n",
        "           fontweight='bold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjPM0vzTA3ur"
      },
      "outputs": [],
      "source": [
        "# going to redistrubute dataset so it's 1) more equal and 2) smaller aka trains faster \n",
        "# each category will get 3000 entries-- ensure all unique descriptions are present once, then random sample for remainder\n",
        "# 3000 chosen bc the highest number of unique desc is 2790 (`3_information_technology`)\n",
        "\n",
        "redist_dfs = []\n",
        "for i, c in enumerate(training_data['category'].drop_duplicates()):\n",
        "    df = training_data.loc[training_data['category'] == c]\n",
        "    df_unique = df.drop_duplicates(subset=['description_en']) # unique descriptions\n",
        "    \n",
        "    # `11_defence` is super tiny so it needs a special case to allow repeat sampling\n",
        "    # if c == '11_defence':\n",
        "    #     df_random = df.sample(n=(5000-len(df_unique)), replace=True, random_state=1)\n",
        "    # else:\n",
        "    #     df_random = df.sample(n=(5000-len(df_unique)), random_state=1)\n",
        "    \n",
        "    df_random = df.sample(n=(7000-len(df_unique)), replace=True, random_state=1)\n",
        "\n",
        "    df = pd.concat([df_unique, df_random])\n",
        "    redist_dfs.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO67dt7GA_Mh"
      },
      "outputs": [],
      "source": [
        "# verifying success\n",
        "training_data = pd.concat(redist_dfs)\n",
        "training_data['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u95dkPBSA_AN"
      },
      "outputs": [],
      "source": [
        "# map categories to numeric values\n",
        "training_data['label'] = training_data['category'].map({'1_facilities_and_construction': 0,\n",
        "                                            '2_professional_services': 1,\n",
        "                                            '3_information_technology': 2,\n",
        "                                            '4_medical': 3,\n",
        "                                            '5_transportation_and_logistics': 4,\n",
        "                                            '6_industrial_products_and_services': 5,\n",
        "                                            '7_travel': 6,\n",
        "                                            '8_security_and_protection': 7,\n",
        "                                            '9_human_capital': 8,\n",
        "                                            '10_office_management': 9,\n",
        "                                            '11_defence': 10})\n",
        "\n",
        "training_data = training_data[['description_en', 'category', 'label']].copy()\n",
        "training_data = training_data.sample(frac=1, random_state=2).reset_index(drop=True)\n",
        "\n",
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liDP04huE3QT"
      },
      "outputs": [],
      "source": [
        "# split data into test/train\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = tf.keras.utils.to_categorical(training_data['label'].values, num_classes=num_classes)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(training_data['description_en'], y, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIzIxxF3FYDy"
      },
      "outputs": [],
      "source": [
        "# creating embeddings\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1\")\n",
        "\n",
        "\n",
        "def get_embeddings(sentences):\n",
        "  '''return BERT-like embeddings of input text\n",
        "  Args:\n",
        "    - sentences: list of strings\n",
        "  Output:\n",
        "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
        "  '''\n",
        "  preprocessed_text = preprocessor(sentences)\n",
        "  return encoder(preprocessed_text)['pooled_output']\n",
        "\n",
        "\n",
        "get_embeddings([\n",
        "    \"Human resources services\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfuTvjErIHHR"
      },
      "outputs": [],
      "source": [
        "# finetuning functions\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def balanced_recall(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced recall metric\n",
        "    recall = TP / (TP + FN)\n",
        "    \"\"\"\n",
        "    recall_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        recall_by_class = recall_by_class + recall\n",
        "    return recall_by_class / y_pred.shape[1]\n",
        "\n",
        "def balanced_precision(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced precision metric\n",
        "    precision = TP / (TP + FP)\n",
        "    \"\"\"\n",
        "    precision_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        precision_by_class = precision_by_class + precision\n",
        "    # return average balanced metric for each class\n",
        "    return precision_by_class / y_pred.shape[1]\n",
        "\n",
        "def balanced_f1_score(y_true, y_pred):\n",
        "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
        "    precision = balanced_precision(y_true, y_pred)\n",
        "    recall = balanced_recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMOopfj7JL4Y"
      },
      "outputs": [],
      "source": [
        "# define preprocess + encoding layers\n",
        "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "x = preprocessor(i)\n",
        "x = encoder(x)\n",
        "x = tf.keras.layers.Dropout(0.2, name=\"dropout\")(x['pooled_output'])\n",
        "x = tf.keras.layers.Dense(num_classes, activation='softmax', name=\"output\")(x)\n",
        "\n",
        "model = tf.keras.Model(i, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaJ1-PX1JadF"
      },
      "outputs": [],
      "source": [
        "# actual finetuning \n",
        "n_epochs = 30\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "      balanced_recall,\n",
        "      balanced_precision,\n",
        "      balanced_f1_score\n",
        "]\n",
        "\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
        "                                                      patience = 3,\n",
        "                                                      restore_best_weights = True)\n",
        "\n",
        "filepath=\"n1-weights-improvement-{epoch:02d}-{balanced_f1_score:.2f}.hdf5\"\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='balanced_f1_score', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "\n",
        "model.compile(optimizer = \"adam\",\n",
        "              loss = \"categorical_crossentropy\",\n",
        "              metrics = METRICS)\n",
        "\n",
        "model_fit = model.fit(x_train, \n",
        "                      y_train, \n",
        "                      epochs = n_epochs,\n",
        "                      validation_data = (x_test, y_test),\n",
        "                      callbacks = [earlystop_callback, checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvAAOj-Xhxpr"
      },
      "outputs": [],
      "source": [
        "x = list(range(1, n_epochs+1))\n",
        "metric_list = list(model_fit.history.keys())\n",
        "num_metrics = int(len(metric_list)/2)\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
        "\n",
        "for i in range(0, num_metrics):\n",
        "  ax[i].plot(x, model_fit.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
        "  ax[i].plot(x, model_fit.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
        "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
        "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
        "  ax[i].legend(loc=\"lower left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4D2zJnbikVN"
      },
      "outputs": [],
      "source": [
        "# test prediction with strongest model\n",
        "test_descs = [\"Other business services not elsewhere\",\n",
        "              \"DIGITAL COMMUNICATIONS EQUIPMENT\",\n",
        "              \"Non-public servant travel - Support core\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xeezkza8ibf4"
      },
      "outputs": [],
      "source": [
        "def predict_class(test_descs):\n",
        "  '''predict class of input text\n",
        "  Args:\n",
        "    - reviews (list of strings)\n",
        "  Output:\n",
        "    - class (list of int)\n",
        "  '''\n",
        "  return [np.argmax(pred) for pred in model.predict(test_descs)]\n",
        "\n",
        "predict_class(test_descs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_ozeS25qmnH"
      },
      "outputs": [],
      "source": [
        "model.save(\"BEST-n1-weights-improvement-22-0.92.hdf5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSG99D1Vkdkz"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need to reload the custom layers so they can be passed to our model on load\n",
        "# finetuning functions\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def balanced_recall(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced recall metric\n",
        "    recall = TP / (TP + FN)\n",
        "    \"\"\"\n",
        "    recall_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        recall_by_class = recall_by_class + recall\n",
        "    return recall_by_class / y_pred.shape[1]\n",
        "\n",
        "def balanced_precision(y_true, y_pred):\n",
        "    \"\"\"This function calculates the balanced precision metric\n",
        "    precision = TP / (TP + FP)\n",
        "    \"\"\"\n",
        "    precision_by_class = 0\n",
        "    # iterate over each predicted class to get class-specific metric\n",
        "    for i in range(y_pred.shape[1]):\n",
        "        y_pred_class = y_pred[:, i]\n",
        "        y_true_class = y_true[:, i]\n",
        "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        precision_by_class = precision_by_class + precision\n",
        "    # return average balanced metric for each class\n",
        "    return precision_by_class / y_pred.shape[1]\n",
        "\n",
        "def balanced_f1_score(y_true, y_pred):\n",
        "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
        "    precision = balanced_precision(y_true, y_pred)\n",
        "    recall = balanced_recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ],
      "metadata": {
        "id": "48BvCAgN8B8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiwtMSDPBhES"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# load dataset for creating test_set\n",
        "og_data = pd.read_csv('rule_based_contracts_v1.csv', dtype='str')\n",
        "\n",
        "\n",
        "# load model\n",
        "model = keras.models.load_model((\"BEST-n1-weights-improvement-22-0.92.hdf5\"), custom_objects={'KerasLayer':hub.KerasLayer,\n",
        "                                                                                                 'balanced_recall': balanced_recall, \n",
        "                                                                                                 'balanced_precision': balanced_precision, \n",
        "                                                                                                 'balanced_f1_score': balanced_f1_score})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_keys = {'1_facilities_and_construction': 0,\n",
        "              '2_professional_services': 1,\n",
        "              '3_information_technology': 2,\n",
        "              '4_medical': 3,\n",
        "              '5_transportation_and_logistics': 4,\n",
        "              '6_industrial_products_and_services': 5,\n",
        "              '7_travel': 6,\n",
        "              '8_security_and_protection': 7,\n",
        "              '9_human_capital': 8,\n",
        "              '10_office_management': 9,\n",
        "              '11_defence': 10}"
      ],
      "metadata": {
        "id": "-0ad8hnJFOH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQlzTi200kHx"
      },
      "outputs": [],
      "source": [
        "# creating a test set\n",
        "no_class = og_data.loc[og_data['category'].isna() & og_data['economic_object_code'].isna()]\n",
        "\n",
        "test_set = no_class[no_class['description_en'].notna()].sample(n=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_ewJGyqkUon"
      },
      "outputs": [],
      "source": [
        "# minimize\n",
        "test_set = test_set[['category', 'description_en']].copy().reset_index(drop=True)\n",
        "test_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, pred in enumerate(model.predict(test_set['description_en'])):\n",
        "  # np.max(pred) * 100\n",
        "  print(pred)\n",
        "  print(np.argmax(pred))\n",
        "  print(np.argsort(pred, axis=0)[-2])\n",
        "  test_set.loc[i, 'category'] = list(class_keys)[np.argmax(pred)]\n",
        "  test_set.loc[i, 'alt_category'] = list(class_keys)[np.argsort(pred, axis=0)[-2]]\n",
        "  test_set.loc[i, 'alt_confidence'] = np.partition(pred.flatten(), -2)[-2]\n",
        "  test_set.loc[i, 'num_category'] = np.argmax(pred)\n",
        "  test_set.loc[i, 'confidence'] = np.max(pred) * 100\n",
        "  print('-----------------')\n"
      ],
      "metadata": {
        "id": "MHIJSvifFVjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set"
      ],
      "metadata": {
        "id": "RyXBOKKfQ2YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set.describe()\n"
      ],
      "metadata": {
        "id": "4vnPHDIkORAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "data_gov_classifier_TF.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}